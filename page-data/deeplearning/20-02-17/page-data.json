{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/20-02-17/","result":{"data":{"markdownRemark":{"html":"<p> 얼마 전 연구실에서 진행중인 실험과 관련해서 depth estimation 코드를 돌려볼 일이 생겨서 depth estimation에 대해 아주 쪼끔 공부를 하게 되었습니다. 이번 포스팅에서는 이에 대해 제가 알게 된 내용과 참고한 논문 코드들을 공유해보도록 하겠습니다. </p>\n<h3>Depth estimation</h3>\n<p> Depth estimation을 한글로 번역하면 '깊이 추정'입니다. 모델에 우리가 2D 사진을 입력값으로 주면, 모델이 이 <strong>사진의 깊이 값을 출력으로 뱉는 것</strong>이 depth estimation이라고 할 수 있습니다. 주로 <strong>'깊이 값이 label로 주어진 이미지'</strong>를 사용하여 학습을 진행하고, 모델은 깊이 값이 주어지지 않은 이미지가 주어졌을 때 깊이 값을 추정하여 실제 label과 비교하는 방식으로 가중치를 업데이트 해가는 supervised learning 방식을 사용하고있습니다. </p>\n<h3>Datasets</h3>\n<p> Depth estimation 모델 학습을 위한 dataset은 depth camera를 통해 수집합니다. 그리고 학습에 자주 사용되는 유명한 dataset들은 NYU dataset과 KITTI dataset이 있습니다. </p>\n<h4>NYU2 dataset</h4>\n<center><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/282fc569f56d0f9ec177d9004f9f1768/21b4d/20-02-17-1.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 24.210526315789476%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABgUlEQVQY0w3F0UsaAQDA4ft79hoxGESNxSqSLDqplpyaZiRuawanyNrupPJO5yxLpexsl0EzCqNwtIc1HCtiqx6ix3LQGj3UQ0S0XsZvPXx8wsp0gp3NJdZ9No5sFm6LIt83gmjuCA5vmq/ya/7d/eXq+obC/CK5mQLLLhe5Visli0h1zcdhxc8PtZPj0jSC9uwJEamFwpiVj4aMXs6S+Zxi2+ygFGyg/MbJxeUVC/llUqqK7JTI+NoxdSvmcB1LzibSbgvxRw9YN3SECe9TNKkOIzZMUpkgLEbJxUbY3xrk00Lf/Srnfy7IJ7MEXA48PV3kZxUSygvm/Y+ZejVIIqVgjtj5UCwjyN4+lLBMJdjMmVzD9lAbk2E//aH3eHpHCY0v8qv6m7DdRsQj3bPzbc7KqmHnXSbGjKYwqb0kGfezW8whfCmkcYk96PoAc1tZuotHPA9FGarvoLN/CiVhcFo9IdArMjogEfV0s1Fxo+5VaIr/JCgGWG2oxWx8yMFbmf9k0QScKwASLgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/282fc569f56d0f9ec177d9004f9f1768/15813/20-02-17-1.webp 190w,\n/static/282fc569f56d0f9ec177d9004f9f1768/1cdb2/20-02-17-1.webp 380w,\n/static/282fc569f56d0f9ec177d9004f9f1768/9046c/20-02-17-1.webp 760w,\n/static/282fc569f56d0f9ec177d9004f9f1768/c89f9/20-02-17-1.webp 1140w,\n/static/282fc569f56d0f9ec177d9004f9f1768/af3f0/20-02-17-1.webp 1280w\" sizes=\"(max-width: 760px) 100vw, 760px\" type=\"image/webp\">\n          <source srcset=\"/static/282fc569f56d0f9ec177d9004f9f1768/a2d4f/20-02-17-1.png 190w,\n/static/282fc569f56d0f9ec177d9004f9f1768/3f520/20-02-17-1.png 380w,\n/static/282fc569f56d0f9ec177d9004f9f1768/3c051/20-02-17-1.png 760w,\n/static/282fc569f56d0f9ec177d9004f9f1768/b5cea/20-02-17-1.png 1140w,\n/static/282fc569f56d0f9ec177d9004f9f1768/21b4d/20-02-17-1.png 1280w\" sizes=\"(max-width: 760px) 100vw, 760px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/282fc569f56d0f9ec177d9004f9f1768/3c051/20-02-17-1.png\" alt=\"20 02 17 1\" title=\"20 02 17 1\" loading=\"lazy\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n  </a>\n    </span><p><i>https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</i></p></center>\n<p> NYU dataset은 <strong>실내 공간</strong>에서의 정지된 사진과 깊이 값이 담겨있는 데이터셋입니다. 뿐만 아니라 가구 각각에 대한 class label도 포함되어 있습니다. <a href=\"https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\">원본 사이트</a> 내 데이터셋에 대한 자세한 설명이 있습니다. </p>\n<h4>KITTI dataset</h4>\n<center><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/0d4149a6aff93d190303656736d24297/21b4d/20-02-17-2.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 29.47368421052632%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlUlEQVQY032OvUtbURiH79J83NyPJPeGJPdWmhiw2ihNjNE2EWIo1LjoYAqlIEhREbRD2jRDRYTSQcFFHAy6SIeugpROThVKaQehbkJbcfAPcGxy83iSCt36gx/PeQ+c5z3SZaPBlePwB2i1WjjNJk1RR9z9499zO78vfpHN53DJLjyKjFfx4Za9uH2iXg8SN2mIR44Q/i+HHw+4N9CDRwgUvyYkckd6y+PGJWQuQam+u87O1hpvVyvs1jf5fPSJ78fHnJ2f8/XbF05+nHB69pO99x+Idtmofp2AaXQoqwqypuITVdoUs1R7t8pmfZv5xVlS6QxDDwqM3U+z/2KO15UqS69qDObHSRfLJHpTmBELw4qiGQF0M4gu6NNVIVY6C6TSzEumFlYYKT0lM1piKFdkbDjLxvIcb2pV8oVx7FgfYSvOnbuD2IkBgpEuAmEbfziCFjJRggFkIfW2hQ+Lk2RyjzFCFulsnufzC5SfTFN+NkPh0QSxnhS3E/1EY73Y3UniyRGi8SR6yBY/tNDM8E1NVCPINYIK/2Kjzs7eAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/0d4149a6aff93d190303656736d24297/15813/20-02-17-2.webp 190w,\n/static/0d4149a6aff93d190303656736d24297/1cdb2/20-02-17-2.webp 380w,\n/static/0d4149a6aff93d190303656736d24297/9046c/20-02-17-2.webp 760w,\n/static/0d4149a6aff93d190303656736d24297/c89f9/20-02-17-2.webp 1140w,\n/static/0d4149a6aff93d190303656736d24297/af3f0/20-02-17-2.webp 1280w\" sizes=\"(max-width: 760px) 100vw, 760px\" type=\"image/webp\">\n          <source srcset=\"/static/0d4149a6aff93d190303656736d24297/a2d4f/20-02-17-2.png 190w,\n/static/0d4149a6aff93d190303656736d24297/3f520/20-02-17-2.png 380w,\n/static/0d4149a6aff93d190303656736d24297/3c051/20-02-17-2.png 760w,\n/static/0d4149a6aff93d190303656736d24297/b5cea/20-02-17-2.png 1140w,\n/static/0d4149a6aff93d190303656736d24297/21b4d/20-02-17-2.png 1280w\" sizes=\"(max-width: 760px) 100vw, 760px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/0d4149a6aff93d190303656736d24297/3c051/20-02-17-2.png\" alt=\"20 02 17 2\" title=\"20 02 17 2\" loading=\"lazy\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n  </a>\n    </span><p><i>http://www.cvlibs.net/datasets/kitti/index.php</i></p></center>\n<p> KITTI dataset은 움직이는 자동차에 카메라를 달고 운행을 하면서 <strong>거리의 영상</strong>을 깊이 값과 함께 담아낸 데이터셋입니다. <a href=\"http://www.cvlibs.net/datasets/kitti/\">원본 사이트</a> 내 데이터셋에 대한 자세한 설명이 있습니다. </p>\n<h3>Depth estimation models</h3>\n<p> Github에서 depth estimation 관련 코드를 찾아서 직접 코드레벨에서 실행을 시켜보았습니다. 먼저 pythone opencv2를 사용하여 직접 촬영한 사진을 로드하여 이 사진에 대해 결과를 prediction 하도록 해보았습니다.</p>\n<p> 그리고 사진 뿐만 아니라 동영상을 realtime으로 prediction 또한 해보고 싶어서 동영상을 로드하여 frame마다 prediction을 진행해보기도 하였습니다. 근데 동영상의 경우에는 prediction 속도가 너무 느려서 결국 사진만 prediction 해보는 것으로 만족하였습니다. (제가 원했던 속도는 gpu로 실행시 초당 15 프레임 정도였는데, 아래의 코드들은 그 정도 속도까지는 나오지 않았습니다.)</p>\n<h4>Densedepth</h4>\n<ul>\n<li><a href=\"https://github.com/ialhashim/DenseDepth\">https://github.com/ialhashim/DenseDepth</a></li>\n</ul>\n<p> Densedepth 코드는 <a href=\"https://arxiv.org/abs/1812.11941\">'High Quality Monocular Depth Estimation via Transfer Learning'</a>라는 논문에서 소개된 코드입니다. 전체적인 모델의 구조는 CNN을 활용한 <strong>standard encoder-decoder architecture</strong>를 가지고 있고, ImageNet dataset으로 학습시킨 DenseNet을 encoder의 pre-trained network로 사용하는 <strong>transfer learning</strong> 방식을 사용하였습니다. 저는 이 논문의 취지를 'transfer learning을 사용한 네트워크가 depth estimation을 상당히 잘 수행하였다.' 정도로 이해했습니다.</p>\n<p>이 코드는 NYU2 dataset으로 학습시킨 모델과 KITTI dataset으로 학습시킨 모델을 둘 다 제공하고있고, TensorFlow 코드와 PyTorch 코드를 모두 제공하고있습니다. 아래는 제가 직접 찍은 사진을 인풋으로 넣어서 얻어낸 출력결과 입니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/16cf40d30c164002924ac63c178418df/21b4d/20-02-17-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.05263157894737%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADY0lEQVQozy3Sa0wbZBTGcRITjCOSzOkuuChFMrJxGREvONI6CotZqAq0MAbBDbqYKMFoxm1A02knlGVdLaUFWhhtuS1FwGWMibQUkXF1hU1mDCTgnMHgMiNFEOfgb1k9X95vvzzveY7f+vo6wyMjzM8v8F2fkyZdNbU1WqwWMxZrA1ariZkZN9f0Mxjlg7gUvSy2GfG4NKyOXWB1XMNyv5o/hzvZGr+HD//BUKunWn0ee0sLSkUJeR/mUlyYh+psIVWVZ5icHEKfep2i7RquiAv49WISnn4xHkcCnqsJLOmELJrL2NwCHz36l28c16g31VBZeZZ6o4ZWay1fdtmw2RopLStldPwGrblOtGFaXNJi7rfL+HsphZU7qSz3pnBPm8zduiofuLm5wc9353C6+uiw22ixNXDiVB4JSRnExsvIzC3CPT2FOdtBwa56DFE1fHvqNHOmHBa0WdzTZfCbScKsVoOX2gI3efDHEjM/3mJ6epTmtjZePJBIXIIM0ZE0Il59h+/dUzS85yT/ORPlL1nQR+i5efJ95suPM5aTz2iWnFuqGrwUfhsbG9x/sMhPs3eYnfsBe1c3+2LeJeFtOWKJnKg3ZIxNTHI5dythHWWhTVTtr8MhLWImP5fBYwX0JJ5m8GOb78vLyx46O9u5fXuCpd9/obunl2cFhxFESgg5mOJNe5Th8UkMJ/pQPG9AE27ii3Cj962lTVTBdUk57bEKvpL/37LHs4LwTRFC0SHOfa6kud3OE9uj2bYrloA9QmKEad5VTHFS2kX4tgtkBRsxRFdTKmhCEdJEdZSRimA9Ldk9PnB1dZXIg1EEPB1I2P4DGOvrkX9QSPzRTCJfT0EQkYhrcJBjaXYC/VWE7dSijlGjizCiDG3gXFgjZUF1NGX2+sCVlRXiRIeJFcXzmlCM8jMVbvc4HR3ttHoL+qSgEOeAg6z0TgKeVOK/41Pi9lxEve8SJSGXOOMt6aOdZnTpX/vAv7wJJalpvHJISHj0y+j0OoaGXF6knwGXg84uOzfdEyQnt/GUv5KgHWoku+vI32slJ8hCzl4L0kAzKu/hPwbX1tZ4K0mCMF5MSWkxXd12RseGGZ+4wcjoEEPDA8wvzCJLv8wLz5zniMBMarCFDIGVrBAb2aHNSHc3UnG87zH4H0tuf871WbcWAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/16cf40d30c164002924ac63c178418df/15813/20-02-17-3.webp 190w,\n/static/16cf40d30c164002924ac63c178418df/1cdb2/20-02-17-3.webp 380w,\n/static/16cf40d30c164002924ac63c178418df/9046c/20-02-17-3.webp 760w,\n/static/16cf40d30c164002924ac63c178418df/c89f9/20-02-17-3.webp 1140w,\n/static/16cf40d30c164002924ac63c178418df/af3f0/20-02-17-3.webp 1280w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/16cf40d30c164002924ac63c178418df/a2d4f/20-02-17-3.png 190w,\n/static/16cf40d30c164002924ac63c178418df/3f520/20-02-17-3.png 380w,\n/static/16cf40d30c164002924ac63c178418df/3c051/20-02-17-3.png 760w,\n/static/16cf40d30c164002924ac63c178418df/b5cea/20-02-17-3.png 1140w,\n/static/16cf40d30c164002924ac63c178418df/21b4d/20-02-17-3.png 1280w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/16cf40d30c164002924ac63c178418df/3c051/20-02-17-3.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h4>monodepth2</h4>\n<ul>\n<li><a href=\"https://github.com/nianticlabs/monodepth2\">https://github.com/nianticlabs/monodepth2</a></li>\n</ul>\n<p> monodepth2 코드는 논문은 읽어보지 않고 코드 레벨에서 실행만 해보았습니다. 이 코드는 KITTI dataset으로 학습시킨 모델만 제공하고있고, 코드는 PyTorch로 구현되었습니다. 아래는 제가 직접 찍은 사진을 인풋으로 넣어서 얻어낸 출력결과 입니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1a297bdfc32e2523c0cfb71a1af4cdd5/21b4d/20-02-17-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.57894736842105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAADrklEQVQ4y22Se1CUVRiH+av+qJkcQS5yiYCE1CUwUAaFMKjUapxmKgeJImG3wIVtWbmtu4tcAoJVlNsCw/0qbULsgoqMCFODl1FrkHEwyW6YWo4lN0vYfTpcHKeZzszzne+cM98zv+89rw1i9Jur6Tmqo75KS+Tut/HzX4v/i55sDlqPnZMzcR+8wTfmQ3RUZ7Bxgw8SHxc+kUbS22dmePgU8dHhaFUfMT8/j82C8KTJwJeNKRwpVBIU8jKubu54r/HC2cWVp1c683pEMCdatDSXqvDy9sTO3o5339mBsauNvj4jSXteJVcteyysPqyhPD+WgiwpEl9fnJ3dcXPzwNHVD1ePYF4JDaPaoKeuqRlFSjoJ8gTU6XKqakswmVtJ/FAkTJVisViWhB/n5LE9OZFd0dtxcHLH4blA7B09We3yLM+ssCX0tTcpqjOTV9FOTnE5WYUHuXB5gJ7jLZi7KsnTRGMoOYDVal0SJufmsHlrIClJuwgICscvYBtrJcGsl/ixysmL9P0p/PH7FUZGznDt2hCjV89y8dJJTKZKeox6mitVdLYUiITLv3xURG+tLePyd5eoaTNSVFFHlr4UbcFB4lVqDuUncaYzF3OzmjaDgsYjCbQblHQ26jDWZFCVH01LRQaWhRouxJyZuSu0D5lnDiv/HT/f/oGrIye48+t5bt4YZmL8HL98f5YfRdIbY4OMjZ7m2wvdjF0ZfFzDtrIBdNJ6ipO/IFvehFxajlrWQp6sHf1eIyUqE3pVJxplPVKZQpCEVtnA4ZRuOg70cirXxMW6IayPhLIdxfjYxBC6QslOxwTi1r2Hems8+t3i9t9Xk7pFwZonY1j1xDZsbOwEK3EQ7wFPJbDHTYXWI4mSt4qwzC3XMD2yii22iYQ4ZVC/M5GpfbHc0imY//pzuG7g9CEtvrYyNq6WESDwFzxvH8UGBzmxL2jJ88+gKaoE66M+bI2vQ7cuFZmPhsFPFdzNFsJaDZMj9TDewFdqNeH2acT6ZJIs0aGSaFFI0sh8KZXCTRp6I/byU24WwrgkHFUX0x8ax7GwJCY+S2DalMmD8Tb+vteHZaKV4bRUsryTKQ1UUbMpmYYgFcdClHSHKemPkHM9Jkp8UyQudLkP/+qq4k7ZPu61ZjM9bODh5ABzlvPMzZ1j7sEQ94/rxfl+blVo+a1cy81SjZg1YlZzuyyDPzt0IkC3MC0IRZ9YLdPicV9szIrlP4LZxb1FrDNiLc6Z+h8mF7FapwSziwn/BU7cA0p5wZjNAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/1a297bdfc32e2523c0cfb71a1af4cdd5/15813/20-02-17-4.webp 190w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/1cdb2/20-02-17-4.webp 380w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/9046c/20-02-17-4.webp 760w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/c89f9/20-02-17-4.webp 1140w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/af3f0/20-02-17-4.webp 1280w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/1a297bdfc32e2523c0cfb71a1af4cdd5/a2d4f/20-02-17-4.png 190w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/3f520/20-02-17-4.png 380w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/3c051/20-02-17-4.png 760w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/b5cea/20-02-17-4.png 1140w,\n/static/1a297bdfc32e2523c0cfb71a1af4cdd5/21b4d/20-02-17-4.png 1280w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/1a297bdfc32e2523c0cfb71a1af4cdd5/3c051/20-02-17-4.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span> </p>\n<p> 사용해 볼만 한 더 좋은 모델이 있다면 추천해주시면 감사하겠습니다.</p>\n<h3>관련 자료들</h3>\n<ul>\n<li>구글의 Depth Prediction: <a href=\"https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html?fbclid=IwAR12YTuDS99mvPse-T0Jmn9TQKwskpxIUcWLQrJmg3J-ZdcRwLbiRVcEBa4\">https://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html?fbclid=IwAR12YTuDS99mvPse-T0Jmn9TQKwskpxIUcWLQrJmg3J-ZdcRwLbiRVcEBa4</a></li>\n<li>구글 AI와 구글 로보틱스의 연구 논문: <a href=\"https://arxiv.org/pdf/1904.04998.pdf\">https://arxiv.org/pdf/1904.04998.pdf</a></li>\n<li>Single View Stereo Matching: <a href=\"https://github.com/lawy623/SVS?fbclid=IwAR1xN_ClQ2vO6Xnmcrnmn_433KW2G96B-VtpIuYNwomq0LEGkbe1Q1M_y4o\">https://github.com/lawy623/SVS?fbclid=IwAR1xN_ClQ2vO6Xnmcrnmn_433KW2G96B-VtpIuYNwomq0LEGkbe1Q1M_y4o</a></li>\n<li>Google PAIR depth maps art and illusions: <a href=\"https://pair-code.github.io/depth-maps-art-and-illusions/art_history_vis/blogpost/blogpost_1.html?fbclid=IwAR2ZKvVTFoxh0f7cFC6_09QsAdKUsxPIZF-rmKAWQqkrndlV_cf_V7cntZE\">https://pair-code.github.io/depth-maps-art-and-illusions/art_history_vis/blogpost/blogpost_1.html?fbclid=IwAR2ZKvVTFoxh0f7cFC6_09QsAdKUsxPIZF-rmKAWQqkrndlV_cf_V7cntZE</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/20-02-17/","title":"Depth Estimation","category":"Deep Learning","date":"2020-02-17"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}