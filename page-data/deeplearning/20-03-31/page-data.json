{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/20-03-31/","result":{"data":{"markdownRemark":{"html":"<p> 딥러닝 생성 모델 중 하나인 variational autoencoder(변이형 오토인코더)에 대해 알아보겠습니다. 'Generative Deep Learning' (O'REILLY) 책을 통해 공부한 내용을 중심으로 포스팅을 작성하였습니다. Additional information 파트를 특히 많이 인용하였습니다. </p>\n<h3 id=\"variational-autoencoder\" style=\"position:relative;\"><a href=\"#variational-autoencoder\" aria-label=\"variational autoencoder permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Variational Autoencoder</h3>\n<h5 id=\"1-vae의-목표\" style=\"position:relative;\"><a href=\"#1-vae%EC%9D%98-%EB%AA%A9%ED%91%9C\" aria-label=\"1 vae의 목표 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. VAE의 목표</h5>\n<p> 우리가 VAE를 통해 이루고자하는 궁극적인 목표는 <strong>실제 데이터의 분포</strong>를 알아내는 것 입니다. 만약 '1만 차원의 사람 사진 데이터'를 생성하고 싶다고 한다면, 사람 사진 데이터가 1만 차원 상에서 어떤 점에 위치해 있는지를 이상적인 확률 분포로써 알아내는 것이 우리의 목표입니다. 이 분포를 p(x)로 표현합니다.</p>\n<h5 id=\"2-데이터의-분포를-어떻게-추정해야-하는가\" style=\"position:relative;\"><a href=\"#2-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%B6%94%EC%A0%95%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94%EA%B0%80\" aria-label=\"2 데이터의 분포를 어떻게 추정해야 하는가 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 데이터의 분포를 어떻게 추정해야 하는가</h5>\n<p> 그렇다면 우리는 이 확률분포 p(x)만 잘 찾아낸다면 데이터를 생성할 수 있게됩니다. 하지만 우리는 1만 차원 상에서 사람 사진 데이터가 어떤 점에 위치하고 있는지 추정할 근거가 없습니다. 1만 차원이라는 공간에는 무수히 많은 점들이 존재하기 때문에 이 공간에 점을 하나하나 다 찍어서 데이터가 잘 생성되는지 확인하는 방법도 먹히지 않을 것입니다. 그래서 우리는 이 데이터들의 특성을 잘 표현하는 표현 벡터 <strong>z를 통해 p(x)를 추정</strong>하게 됩니다.  </p>\n<h5 id=\"3-표현-벡터-z의-분포를-어떻게-추정해야-하는가\" style=\"position:relative;\"><a href=\"#3-%ED%91%9C%ED%98%84-%EB%B2%A1%ED%84%B0-z%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%B6%94%EC%A0%95%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94%EA%B0%80\" aria-label=\"3 표현 벡터 z의 분포를 어떻게 추정해야 하는가 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 표현 벡터 z의 분포를 어떻게 추정해야 하는가</h5>\n<p> 표현 벡터 z를 잘 설정하여 이를 VAE의 decoder에 전달해주면 decoder는 알아서 1만 차원 상에 데이터가 존재하는 점을 우리에게 알려줄 것입니다. 하지만 여기서 또 문제가 생깁니다. 표현 벡터 z가 이상적으로 어떻게 분포하고 있는지 우리는 알 수 없습니다. <strong>표현 벡터 z의 확률 분포 p(z)를 추정</strong>하기 위해 encoder를 통해 원본 데이터를 인코딩을 하면 이를 p(z|x)로 생각할 수 있는데, 우리는 어떤 것이 이상적인 p(z|x)인지 알 수 없습니다. (= True posterior를 알 수 없음) 이 문제를 해결하기 위해 <strong>variational inference</strong>라는 방법을 사용합니다. </p>\n<h5 id=\"4-variational-inference\" style=\"position:relative;\"><a href=\"#4-variational-inference\" aria-label=\"4 variational inference permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Variational inference</h5>\n<p> variational inference란, 우리가 이상적인 확률 분포를 모르지만 이를 추정하기 위해서 다루기 쉬운 분포를 가정하고 이 확률분포의 모수(평균, 표준편차)를 바꿔가며, 이상적인 확률분포에 근사하게 만들어 그 확률 분포를 대신 사용하는 방법을 말합니다. VAE에서 우리는 이상적인 p(z|x)를 알지 못하므로, 이에 대해 표준정규분포 q(z|x)를 정의하여 q(z|x)의 모수를 바꿔가며 p(z|x)에 근사시킵니다. 학습이 진행되면 q(z|x)가 p(z|x)에 어느정도 근사되며, 그 결과로 표현벡터 z는 우리가 다룰 수 있는 값이 됩니다.</p>\n<h3 id=\"vae를-수식으로-이해\" style=\"position:relative;\"><a href=\"#vae%EB%A5%BC-%EC%88%98%EC%8B%9D%EC%9C%BC%EB%A1%9C-%EC%9D%B4%ED%95%B4\" aria-label=\"vae를 수식으로 이해 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VAE를 수식으로 이해</h3>\n<p> 위의 흐름을 딥러닝 모델로 구현하기 위해서는 먼저 수식으로 loss function을 정의해야 하고, 이후 loss function을 줄이는 방향으로 학습을 진행할 수 있어야 합니다. 일단 우리가 궁극적으로 알고싶은 것은 p(x)이기 때문에 식 전개를 p(x)에 log를 취한 값인 log(p(x))부터 시작합시다. (전개 과정은 <a href=\"https://datascienceschool.net/view-notebook/c5248de280a64ae2a96c1d4e690fdf79/\">데이터 사이언스 스쿨</a> 포스팅에 상세하게 나와있기 때문에 이를 참고하시길 바랍니다.) 그래서 log(p(x))에 대한 식을 쭉 전개하면 최종적으로는 아래와 같이 도출됩니다. </p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mi>L</mi><mi>B</mi><mi>O</mi><mo stretchy=\"false\">(</mo><mi>ϕ</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy=\"false\">(</mo><mi>z</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">log(p(x))=ELBO(\\phi)+KL(q_{\\phi}(z|x)||p(z))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathnormal\">L</span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">ϕ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>E</mi><mi>L</mi><mi>B</mi><mi>O</mi><mo stretchy=\"false\">(</mo><mi>ϕ</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi mathvariant=\"double-struck\">E</mi><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy=\"false\">(</mo><mi>z</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">∣</mi><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>−</mo><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy=\"false\">(</mo><mi>z</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">ELBO(\\phi)=\\mathbb{E}_{q_{\\phi}(z|x)}[log(p(x|z))]-KL(q_{\\phi}(z|x)||p(z))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathnormal\">L</span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">ϕ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1332799999999998em;vertical-align:-0.38327999999999984em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">E</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29011428571428566em;\"><span></span></span></span></span></span></span><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\">x</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.38327999999999984em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.036108em;vertical-align:-0.286108em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span>\n<p> log(p(x))는 bound 되어있는 값이어서 <strong>KLD를 최소화 한다는 것의 의미는 ELBO를 최대화</strong>한다는 것과 동일합니다. 즉, 확률 분포 p(x)를 알아내는 것은 ELBO를 최대화하는 것입니다. 결론적으로는 loss function을 ELBO를 최대화 하도록 정의하면 됩니다. 하지만 주로 loss function을 정의할 때는 loss를 최소화하는 방향으로 학습이 진행되게 합니다. 그래서 ELBO를 최대화 하는 것이 아닌 -ELBO를 최소화하는 방향으로 학습을 진행되도록 loss를 -ELBO로 정의합니다. 최종 loss는 아래와 같습니다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"script\">L</mi><mrow><mi>θ</mi><mo separator=\"true\">,</mo><mi>ϕ</mi><mo separator=\"true\">;</mo><msup><mi>x</mi><mi>i</mi></msup></mrow></msub><mo>=</mo><mo>−</mo><msub><mi mathvariant=\"double-struck\">E</mi><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy=\"false\">(</mo><mi>z</mi><mi mathvariant=\"normal\">∣</mi><msup><mi>x</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mi>i</mi></msup><mi mathvariant=\"normal\">∣</mi><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mo>+</mo><mi>K</mi><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo stretchy=\"false\">(</mo><mi>z</mi><mi mathvariant=\"normal\">∣</mi><msup><mi>x</mi><mi>i</mi></msup><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal L_{\\theta,\\phi;x^i}=-\\mathbb E_{q_{\\phi}(z|x^i)}[log(p_{\\theta}(x^i|z))]+KL(q_{\\phi}(z|x^i)||p(z))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.004598em;vertical-align:-0.321268em;\"></span><span class=\"mord\"><span class=\"mord mathcal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5148400000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\">ϕ</span><span class=\"mpunct mtight\">;</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7570857142857143em;\"><span style=\"top:-2.786em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.321268em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2629039999999998em;vertical-align:-0.3882399999999999em;\"></span><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathbb\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5148400000000004em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29011428571428566em;\"><span></span></span></span></span></span></span><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord mtight\">∣</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7570857142857143em;\"><span style=\"top:-2.786em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3882399999999999em;\"><span></span></span></span></span></span></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.160772em;vertical-align:-0.286108em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">∣</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span>\n<p> 식의 앞단(-E...)은 입력데이터와 출력데이터의 차이에 대한 loss라는 의미에서 <strong>reconstruction error</strong>라고 하고, 식의 뒷단(KL...)은 z의 값이 가우시안 정규분포를 따르도록 강제한다는 의미에서 <strong>regularization error</strong>라고 합니다. </p>\n<h3 id=\"additional-information\" style=\"position:relative;\"><a href=\"#additional-information\" aria-label=\"additional information permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Additional information</h3>\n<h5 id=\"reparametrization-trick\" style=\"position:relative;\"><a href=\"#reparametrization-trick\" aria-label=\"reparametrization trick permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reparametrization trick</h5>\n<p> z를 샘플링 하는 방법에 대해, 코딩을 통해 backpropagation 알고리즘이 잘 작동하도록 사용할 수 있도록 손 봐준 것이 reparametrization trick입니다. 원래 평균이 mu이고 표준편차가 sigma인 가우시안 정규분포에서 z를 샘플링하는 것은 아래 식으로 표현됩니다.</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi>z</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msup><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">z^{i,l}\\sim N(\\mu_i,\\sigma_i^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8991079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p> 하지만 이렇게 샘플링을 하게 되면 backpropagation이 불가능합니다. 그래서 z 샘플링 방법을 아래와 같이 조금 수정을 합니다. 그러면 z는 원래의 확률적 특성을 보존한 채로 샘플링이 가능하면서 backpropagation 또한 가능하게 됩니다. (epsilon은 N(0,1) 가우시안 정규분포에서 샘플링한 값 입니다.)</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi>z</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msup><mo>=</mo><msub><mi>μ</mi><mi>i</mi></msub><mo>+</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>⊙</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">z^{i,l}= \\mu_i + \\sigma_i^2 \\odot \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8991079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1111079999999998em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641079999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⊙</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span></span></span>\n<h5 id=\"kl-divergence\" style=\"position:relative;\"><a href=\"#kl-divergence\" aria-label=\"kl divergence permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>KL Divergence</h5>\n<p> KLD는 간단하게 얘기하자면, 한 확률 분포가 다른 분포와 얼마나 다른지 측정하는 도구입니다. 그래서 이는 VAE에서, 평균이 mu이고 분산이 log_var로 추정된 정규분포가 표준 정규분포와 얼마나 다른지 측정할 때 사용됩니다. 손실함수에 KLD를 추가하면 잠재공간에 포인트를 선택할 때 사용할 수 있는 잘 정의된 분포를 가지게 됩니다. 이 분포에서 샘플링 하면 VAE가 바라보고 있는 영역안의 포인트를 선택할 가능성이 매우 높습니다. 그리고 KLD는 모든 인코딩된 분포를 표준 정규 분포에 가깝도록 강제합니다. 이로 인해 포인트 군집 사이에 큰 간격이 생길 가능성이 적습니다.</p>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<ul>\n<li>ratsgo's blog - <a href=\"https://ratsgo.github.io/generative%20model/2018/01/27/VAE/\">Variational Autoencoder</a></li>\n<li>데이비드 포스터 - Generative Deep Learning (O'REILLY)</li>\n<li>이활석님 '오토인코더의 모든것' - <a href=\"https://www.youtube.com/watch?v=o_peo6U7IRM\">https://www.youtube.com/watch?v=o_peo6U7IRM</a></li>\n<li>데이터 사이언스 스쿨 - <a href=\"https://datascienceschool.net/view-notebook/c5248de280a64ae2a96c1d4e690fdf79/\">https://datascienceschool.net/view-notebook/c5248de280a64ae2a96c1d4e690fdf79/</a></li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#variational-autoencoder\">Variational Autoencoder</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#1-vae%EC%9D%98-%EB%AA%A9%ED%91%9C\">1. VAE의 목표</a></li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#2-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%B6%94%EC%A0%95%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94%EA%B0%80\">2. 데이터의 분포를 어떻게 추정해야 하는가</a></li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#3-%ED%91%9C%ED%98%84-%EB%B2%A1%ED%84%B0-z%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A5%BC-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%B6%94%EC%A0%95%ED%95%B4%EC%95%BC-%ED%95%98%EB%8A%94%EA%B0%80\">3. 표현 벡터 z의 분포를 어떻게 추정해야 하는가</a></li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#4-variational-inference\">4. Variational inference</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#vae%EB%A5%BC-%EC%88%98%EC%8B%9D%EC%9C%BC%EB%A1%9C-%EC%9D%B4%ED%95%B4\">VAE를 수식으로 이해</a></li>\n<li>\n<p><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#additional-information\">Additional information</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#reparametrization-trick\">Reparametrization trick</a></li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#kl-divergence\">KL Divergence</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/20-03-31-Variational%20Autoencodermd/#reference\">Reference</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/20-03-31/","title":"Variational Autoencoder","category":"Deep Learning","date":"2020-03-31"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}