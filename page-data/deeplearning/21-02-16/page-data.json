{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/21-02-16/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>tf.gradient와 tf.train.Optimizer.compute_gradient의 TensorFlow 공식 문서 설명을 번역하고, Stackoverflow를 참고하여 이들의 차이점을 정리합니다.</p>\n</blockquote>\n<h3 id=\"tfgradient\" style=\"position:relative;\"><a href=\"#tfgradient\" aria-label=\"tfgradient permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tf.gradient</h3>\n<p><code class=\"language-text\">xs</code>에 관한 <code class=\"language-text\">ys</code> 의 symbolic derivative를 계산합니다. 텐서 리스트를 반환(return)하며, 각 텐서는 <code class=\"language-text\">sum(dy/dx)</code>의 계산 결과를 의미합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">tf<span class=\"token punctuation\">.</span>gradients<span class=\"token punctuation\">(</span>\n    ys<span class=\"token punctuation\">,</span> xs<span class=\"token punctuation\">,</span> grad_ys<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">'gradients'</span><span class=\"token punctuation\">,</span> colocate_gradients_with_ops<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n    gate_gradients<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> aggregation_method<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> stop_gradients<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n    unconnected_gradients<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>UnconnectedGradients<span class=\"token punctuation\">.</span>NONE\n<span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">ys</code>와 <code class=\"language-text\">xs</code>는 텐서 혹은 텐서 리스트입니다. <code class=\"language-text\">grad_ys</code>는 텐서 리스트 타입이며 <code class=\"language-text\">ys</code>의 초기 gradient 값을 가집니다. 그러므로 <code class=\"language-text\">ys</code>와 같은 길이 값을 갖습니다.</p>\n<p><code class=\"language-text\">stop_gradients</code>는  <code class=\"language-text\">xs</code>에 대해 Constant로 취급되어야 할 텐서 혹은 텐서 리스트입니다. 이 텐서 값들은 학습 과정 중에 역전파되지 않습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">a <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> <span class=\"token number\">2</span> <span class=\"token operator\">*</span> a\ng <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>gradients<span class=\"token punctuation\">(</span>a <span class=\"token operator\">+</span> b<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> stop_gradients<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>위의 예시에서는 <code class=\"language-text\">g</code>는 [1.0, 1.0]의 계산 결과를 갖습니다. <code class=\"language-text\">tf.gradients(a + b, [a, b])</code> 라고 코드를 바꾼다면 [3.0, 1.0]의 계산 결과를 갖게 됩니다. 아래의 예시는 위의 예시와 동일한 의미를 가집니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">a <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>stop_gradient<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>stop_gradient<span class=\"token punctuation\">(</span><span class=\"token number\">2</span> <span class=\"token operator\">*</span> a<span class=\"token punctuation\">)</span>\ng <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>gradients<span class=\"token punctuation\">(</span>a <span class=\"token operator\">+</span> b<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><code class=\"language-text\">tf.stop_gradient</code>는 <strong>그래프를 빌드하는 과정</strong>에 있어서 사용하는 대신, <code class=\"language-text\">stop_gradients</code> 인자는 이미 <strong>그래프가 빌드된 이후</strong>에도 gradient 계산을 멈추는 방법을 제공합니다.</p>\n<h3 id=\"tftrainoptimizercompute_gradients\" style=\"position:relative;\"><a href=\"#tftrainoptimizercompute_gradients\" aria-label=\"tftrainoptimizercompute_gradients permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tf.train.Optimizer.compute_gradients</h3>\n<p> <code class=\"language-text\">var_list</code>에 관한 <code class=\"language-text\">loss</code>의 gradient를 계산합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">compute_gradients<span class=\"token punctuation\">(</span>\n    loss<span class=\"token punctuation\">,</span> var_list<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> gate_gradients<span class=\"token operator\">=</span>GATE_OP<span class=\"token punctuation\">,</span> aggregation_method<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n    colocate_gradients_with_ops<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> grad_loss<span class=\"token operator\">=</span><span class=\"token boolean\">None</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>이 함수는 <code class=\"language-text\">minimize()</code>의 첫 번째 파트로 사용되기도 합니다. <code class=\"language-text\">var_list</code> 내 variable에 대한 gradient를 (gradient, variable) 페어로 이루어진 리스트에 담아 반환(return)합니다.</p>\n<h3 id=\"차이점\" style=\"position:relative;\"><a href=\"#%EC%B0%A8%EC%9D%B4%EC%A0%90\" aria-label=\"차이점 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>차이점</h3>\n<p><code class=\"language-text\">tf.train.Optimizer.compute_gradients</code>(이하 <code class=\"language-text\">compute_gradients</code>)는 <code class=\"language-text\">tf.gradients</code>를 래핑합니다. 따라서 추가적인 asserts를 가지고 있습니다. <code class=\"language-text\">compute_gradients</code>는 <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py\">실제 코드</a>상에서 <code class=\"language-text\">tf.gradients</code>를 <code class=\"language-text\">gradients.gradients</code>라는 이름으로 호출하여 사용하고, 추적 또는 디버깅과 같은 보조 목적으로의 서브클래스로 확장 가능한 기능을 허용합니다.</p>\n<p>반환하는 값에 대해서 비교를 하자면 <code class=\"language-text\">compute_gradients</code>는 (gradient, variable) 페어로 이루어진 리스트를 반환합니다. 이 때에 variable은 항상 존재하지만 gradient는 존재하지 않을 수 있습니다. 만약 variable에 대한 gradient 변화를 적용하려 한다면 <code class=\"language-text\">compute_gradients</code>를 통해 반환된 리스트를 그대로 가져다가 가중치 업데이트에 사용하면 됩니다. </p>\n<p>하지만 <code class=\"language-text\">tf.gradients</code>는 오직 각 variable에 대한 <code class=\"language-text\">sum(dy/dx)</code> 계산 결과만을 반환하기 때문에 variable 리스트가 추가적으로 제공되어야 가중치 업데이트에 사용할 수 있습니다.</p>\n<p>두 가지 방법은 아래 예시처럼 사용이 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\">### Approach 1 ###</span>\nvariable_list <span class=\"token operator\">=</span> desired_list_of_variables\ngradients <span class=\"token operator\">=</span> optimizer<span class=\"token punctuation\">.</span>compute_gradients<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span>var_list<span class=\"token operator\">=</span>variable_list<span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>apply_gradients<span class=\"token punctuation\">(</span>gradients<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">### Approach 2 ###</span>\nvariable_list <span class=\"token operator\">=</span> desired_list_of_variables\ngradients <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>gradients<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">,</span> var_list<span class=\"token operator\">=</span>variable_list<span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>apply_gradients<span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>gradients<span class=\"token punctuation\">,</span> variable_list<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/gradients\">TensorFlow - tf.gradients</a></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer#compute_gradients\">TensorFlow - tf.compat.v1.train.Optimizer</a></li>\n<li><a href=\"https://stackoverflow.com/questions/40539938/whats-the-difference-between-optimizer-compute-gradient-and-tf-gradients-in\">What's the difference between optimizer.compute_gradient() and tf.gradients() in tensorflow?</a></li>\n<li><a href=\"https://stackoverflow.com/questions/45347275/what-is-the-difference-between-tf-gradients-and-tf-train-optimizer-compute-gradi\">What is the difference between tf.gradients and tf.train.Optimizer.compute_gradient?</a></li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/21-02-16-tf.gradient%EC%99%80%20tf.train.Optimizer.compute_gradient%EC%9D%98%20%EC%B0%A8%EC%9D%B4%EC%A0%90/#tfgradient\">tf.gradient</a></li>\n<li><a href=\"/MachineLearning/21-02-16-tf.gradient%EC%99%80%20tf.train.Optimizer.compute_gradient%EC%9D%98%20%EC%B0%A8%EC%9D%B4%EC%A0%90/#tftrainoptimizercompute_gradients\">tf.train.Optimizer.compute_gradients</a></li>\n<li><a href=\"/MachineLearning/21-02-16-tf.gradient%EC%99%80%20tf.train.Optimizer.compute_gradient%EC%9D%98%20%EC%B0%A8%EC%9D%B4%EC%A0%90/#%EC%B0%A8%EC%9D%B4%EC%A0%90\">차이점</a></li>\n<li><a href=\"/MachineLearning/21-02-16-tf.gradient%EC%99%80%20tf.train.Optimizer.compute_gradient%EC%9D%98%20%EC%B0%A8%EC%9D%B4%EC%A0%90/#reference\">Reference</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/21-02-16/","title":"tf.gradient와 tf.train.Optimizer.compute_gradient","category":"Deep Learning","date":"2021-02-16"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}